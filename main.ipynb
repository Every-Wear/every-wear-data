{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aff7982",
   "metadata": {},
   "source": [
    "# Matrix Factorization Model\n",
    "\n",
    "- https://velog.io/@vvakki_/Matrix-Factorization-2\n",
    "- https://youtu.be/Z49JNxS4vsc\n",
    "- https://angeloyeo.github.io/2019/08/01/SVD.html (SVD) -> 근데 SGD를 사용하는게 일반적이라고 함, SVD는 현실적으로 불가능에 가깝다 ㅇㅇ\n",
    "- MF(Matrix Factorization): 행렬분해로 추천시스템에서 사용자, 아이템의 관계를 가장 잘 설명하는 P, Q행렬로 분해하는 것을 의미한다.\n",
    "\n",
    "- 가상의 사용자 A, B, C\n",
    "- 의류 아이템 X, Y, Z\n",
    "\n",
    "![](./imgs/img1.png)\n",
    "\n",
    "- 위 행렬은 사용자 A가 의류 아이템 X와 Z를 구매했음을 나타내며, 사용자 B는 의류 아이템 X와 Y를, 사용자 C는 의류 아이템 Y와 Z를 구매했음을 나타냅니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dac3c04d",
   "metadata": {},
   "source": [
    "### SVD를 사용하여 행렬 분해하기\n",
    "\n",
    "1. 사용자-아이템 행렬을 SVD를 통해 세 개의 작은 행렬로 분해합니다. 일반적으로 행렬 A는 U, Σ, V^T 세 개의 행렬로 분해됩니다.\n",
    "2. U는 사용자 특성 벡터를 담은 행렬이며, 사용자의 차원을 나타냅니다.\n",
    "3. Σ는 특이값을 대각 성분으로 가지는 대각 행렬이며, 행렬의 크기는 사용자와 아이템의 개수에 따라 달라집니다.\n",
    "4. V^T는 아이템 특성 벡터를 담은 행렬이며, 아이템의 차원을 나타냅니다.\n",
    "\n",
    "### 특성 벡터 학습하기\n",
    "\n",
    "1. SVD를 통해 얻은 U와 V^T 행렬을 사용하여 사용자와 아이템 간의 특성 벡터를 학습합니다.\n",
    "2. 일반적으로, U와 V^T의 특정 행은 각각 사용자와 아이템에 대한 특성 벡터를 나타냅니다.\n",
    "\n",
    "### 새로운 사용자에게 추천하기\n",
    "\n",
    "1. 새로운 사용자에게 의류 아이템을 추천할 때, 해당 사용자의 특성 벡터를 예측하고, 다른 사용자나 아이템과의 유사도를 계산하여 추천할 아이템을 결정합니다.\n",
    "2. 유사도는 벡터 간의 거리나 내적 등의 측정 방법으로 계산할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e70d5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자 특성 벡터:\n",
      "사용자 A: [-0.57735027  0.40824829  0.70710678]\n",
      "사용자 B: [-0.57735027  0.40824829 -0.70710678]\n",
      "사용자 C: [-0.57735027 -0.81649658  0.        ]\n",
      "\n",
      "아이템 특성 벡터:\n",
      "아이템 X: [-0.57735027  0.81649658  0.        ]\n",
      "아이템 Y: [-0.57735027 -0.40824829 -0.70710678]\n",
      "아이템 Z: [-0.57735027 -0.40824829  0.70710678]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 사용자-아이템 행렬 정의\n",
    "user_item_matrix = np.array([\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 1],\n",
    "])\n",
    "\n",
    "# SVD 적용\n",
    "U, s, VT = np.linalg.svd(user_item_matrix)\n",
    "\n",
    "# 사용자 특성 벡터\n",
    "user_features = U\n",
    "print(\"사용자 특성 벡터:\")\n",
    "for i in range(user_features.shape[0]):\n",
    "    print(f\"사용자 {chr(ord('A') + i)}: {user_features[i]}\")\n",
    "\n",
    "# 아이템 특성 벡터\n",
    "item_features = VT.T\n",
    "print(\"\\n아이템 특성 벡터:\")\n",
    "for i in range(item_features.shape[0]):\n",
    "    print(f\"아이템 {chr(ord('X') + i)}: {item_features[i]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "992b67b0",
   "metadata": {},
   "source": [
    "- 위의 예시에서 가상의 사용자 A, B, C와 의류 아이템 X, Y, Z에 대한 특성 벡터를 얻을 수 있음\n",
    "- SVD를 통해 얻은 특성 벡터는 사용자와 아이템 간의 상관 관계를 나타내며, 추천 시스템 등에 활용할 수 있음\n",
    "- 실제 데이터에 SVD를 적용할 때에도 비슷한 방식으로 특성 벡터를 구할 수 있음\n",
    "- 스칼라 값 자체가 더 중요하지 백테 방향 (양수 음수)는 중요하지 않음\n",
    "\n",
    "\n",
    "### Collaborative Filtering Approach\n",
    "\n",
    "1. neighborhood methods\n",
    "- 아이템 끼리, 사용자 끼리 관계 계산 후 가장 가까운 거리 (사용자 또는 아이템)을 추천\n",
    "\n",
    "2. latent factor models\n",
    "- rating pattern, factor를 추출하는 방식\n",
    "- 해석하기 어려운 factor가 추출될 수 있음\n",
    "- **이 경우를 matrix factorization method를 활용할 수 있다**\n",
    "- cold start 의 문제, 기존 데이터가 없고 완전 신규 유저 문제\n",
    "\n",
    "\n",
    "### 위 예시에 이어서 새로운 유저 유입 & 예제\n",
    "\n",
    "![](./imgs/img2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb0f66bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자 특성 벡터:\n",
      "사용자 A: [-0.57453835  0.16446442 -0.70710678 -0.37796447]\n",
      "사용자 B: [-0.57453835  0.16446442  0.70710678 -0.37796447]\n",
      "사용자 C: [-4.75963149e-01 -7.94104488e-01  1.11022302e-16  3.77964473e-01]\n",
      "사용자 D: [-3.36556771e-01  5.61516668e-01 -2.22044605e-16  7.55928946e-01]\n",
      "\n",
      "아이템 특성 벡터:\n",
      "아이템 X: [-0.70710678  0.70710678 -0.        ]\n",
      "아이템 Y: [-0.5        -0.5         0.70710678]\n",
      "아이템 Z: [-0.5        -0.5        -0.70710678]\n",
      "====================================================\n",
      "user_d_features >> [-3.36556771e-01  5.61516668e-01 -2.22044605e-16  7.55928946e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 사용자-아이템 행렬 정의\n",
    "user_item_matrix = np.array([[1, 0, 1],\n",
    "                             [1, 1, 0],\n",
    "                             [0, 1, 1],\n",
    "                             [1, 0, 0]])\n",
    "\n",
    "# SVD 적용\n",
    "U, s, VT = np.linalg.svd(user_item_matrix)\n",
    "\n",
    "# 사용자 특성 벡터\n",
    "user_features = U\n",
    "print(\"사용자 특성 벡터:\")\n",
    "for i in range(user_features.shape[0]):\n",
    "    print(f\"사용자 {chr(ord('A') + i)}: {user_features[i]}\")\n",
    "\n",
    "# 아이템 특성 벡터\n",
    "item_features = VT.T\n",
    "print(\"\\n아이템 특성 벡터:\")\n",
    "for i in range(item_features.shape[0]):\n",
    "    print(f\"아이템 {chr(ord('X') + i)}: {item_features[i]}\")\n",
    "\n",
    "print(\"====================================================\")\n",
    "\n",
    "user_d_features = user_features[3]\n",
    "print(f\"user_d_features >> {user_d_features}\")\n",
    "\n",
    "# # 다른 아이템에 대한 추천, 4 by 3 내적으로 인해 제외\n",
    "# item_scores = np.dot(user_d_features[1:], VT)\n",
    "# print(f\"item_scores >> {item_scores}\")\n",
    "\n",
    "# recommended_items = np.argsort(item_scores)[::-1]  # 추천 아이템을 점수의 내림차순으로 정렬합니다.\n",
    "# print(f\"recommended_items >> {recommended_items}\")\n",
    "# recommended_items = [chr(ord('X') + item_index) for item_index in recommended_items]\n",
    "# print(recommended_items)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d8df11e",
   "metadata": {},
   "source": [
    "### 아래 부터가 찐, SGD 활용, Collaborative Filtering Approach의 latent factor models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fc042a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; cost = 1.1079\n",
      "Iteration: 20 ; cost = 0.8494\n",
      "Iteration: 30 ; cost = 0.6812\n",
      "Iteration: 40 ; cost = 0.5510\n",
      "Iteration: 50 ; cost = 0.4509\n",
      "Iteration: 60 ; cost = 0.3761\n",
      "Iteration: 70 ; cost = 0.3204\n",
      "Iteration: 80 ; cost = 0.2780\n",
      "Iteration: 90 ; cost = 0.2449\n",
      "Iteration: 100 ; cost = 0.2185\n",
      "[[0.98769979 2.84597234 2.17617542 1.13005279 2.94194643]\n",
      " [1.92352147 0.56290515 2.55842729 1.74797987 0.85477717]\n",
      " [1.01778402 2.13965893 5.96520207 4.76524356 2.7625116 ]\n",
      " [1.04882045 2.15540713 4.51881071 3.87763933 4.05656033]\n",
      " [2.04353085 0.95668995 5.16991124 3.80932237 0.31702405]\n",
      " [4.94272182 0.96416527 4.95294799 4.12326927 0.72202091]\n",
      " [4.72636954 1.0010211  1.14396956 0.7503631  1.08591756]]\n"
     ]
    }
   ],
   "source": [
    "# https://eda-ai-lab.tistory.com/528\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Base code : https://yamalab.tistory.com/92\n",
    "class MatrixFactorization():\n",
    "    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n",
    "        \"\"\"\n",
    "        :param R: rating matrix\n",
    "        :param k: latent parameter\n",
    "        :param learning_rate: alpha on weight update\n",
    "        :param reg_param: beta on weight update\n",
    "        :param epochs: training epochs\n",
    "        :param verbose: print status\n",
    "        \"\"\"\n",
    "\n",
    "        self._R = R\n",
    "        self._num_users, self._num_items = R.shape\n",
    "        self._k = k\n",
    "        self._learning_rate = learning_rate\n",
    "        self._reg_param = reg_param\n",
    "        self._epochs = epochs\n",
    "        self._verbose = verbose\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : Update matrix latent weight and bias\n",
    "\n",
    "        참고: self._b에 대한 설명\n",
    "        - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n",
    "        - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n",
    "\n",
    "        :return: training_process\n",
    "        \"\"\"\n",
    "\n",
    "        # init latent features\n",
    "        self._P = np.random.normal(size=(self._num_users, self._k))\n",
    "        self._Q = np.random.normal(size=(self._num_items, self._k))\n",
    "\n",
    "        # init biases\n",
    "        self._b_P = np.zeros(self._num_users)\n",
    "        self._b_Q = np.zeros(self._num_items)\n",
    "        self._b = np.mean(self._R[np.where(self._R != 0)])\n",
    "\n",
    "        # train while epochs\n",
    "        self._training_process = []\n",
    "        for epoch in range(self._epochs):\n",
    "            # rating이 존재하는 index를 기준으로 training\n",
    "            xi, yi = self._R.nonzero()\n",
    "            for i, j in zip(xi, yi):\n",
    "                self.gradient_descent(i, j, self._R[i, j])\n",
    "            cost = self.cost()\n",
    "            self._training_process.append((epoch, cost))\n",
    "\n",
    "            # print status\n",
    "            if self._verbose == True and ((epoch + 1) % 10 == 0):\n",
    "                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n",
    "\n",
    "\n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        compute root mean square error\n",
    "        :return: rmse cost\n",
    "        \"\"\"\n",
    "\n",
    "        # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n",
    "        # 참고: http://codepractice.tistory.com/90\n",
    "        xi, yi = self._R.nonzero()\n",
    "        # predicted = self.get_complete_matrix()\n",
    "        cost = 0\n",
    "        for x, y in zip(xi, yi):\n",
    "            cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n",
    "        return np.sqrt(cost/len(xi))\n",
    "\n",
    "\n",
    "    def gradient(self, error, i, j):\n",
    "        \"\"\"\n",
    "        gradient of latent feature for GD\n",
    "\n",
    "        :param error: rating - prediction error\n",
    "        :param i: user index\n",
    "        :param j: item index\n",
    "        :return: gradient of latent feature tuple\n",
    "        \"\"\"\n",
    "\n",
    "        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n",
    "        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n",
    "        return dp, dq\n",
    "\n",
    "\n",
    "    def gradient_descent(self, i, j, rating):\n",
    "        \"\"\"\n",
    "        graident descent function\n",
    "\n",
    "        :param i: user index of matrix\n",
    "        :param j: item index of matrix\n",
    "        :param rating: rating of (i,j)\n",
    "        \"\"\"\n",
    "\n",
    "        # get error\n",
    "        prediction = self.get_prediction(i, j)\n",
    "        error = rating - prediction\n",
    "\n",
    "        # update biases\n",
    "        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n",
    "        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n",
    "\n",
    "        # update latent feature\n",
    "        dp, dq = self.gradient(error, i, j)\n",
    "        self._P[i, :] += self._learning_rate * dp\n",
    "        self._Q[j, :] += self._learning_rate * dq\n",
    "\n",
    "\n",
    "    def get_prediction(self, i, j):\n",
    "        \"\"\"\n",
    "        get predicted rating: user_i, item_j\n",
    "        :return: prediction of r_ij\n",
    "        \"\"\"\n",
    "        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n",
    "\n",
    "\n",
    "    def get_complete_matrix(self):\n",
    "        \"\"\"\n",
    "        computer complete matrix PXQ + P.bias + Q.bias + global bias\n",
    "\n",
    "        - PXQ 행렬에 b_P[:, np.newaxis]를 더하는 것은 각 열마다 bias를 더해주는 것\n",
    "        - b_Q[np.newaxis:, ]를 더하는 것은 각 행마다 bias를 더해주는 것\n",
    "        - b를 더하는 것은 각 element마다 bias를 더해주는 것\n",
    "\n",
    "        - newaxis: 차원을 추가해줌. 1차원인 Latent들로 2차원의 R에 행/열 단위 연산을 해주기위해 차원을 추가하는 것.\n",
    "\n",
    "        :return: complete matrix R^\n",
    "        \"\"\"\n",
    "        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)\n",
    "\n",
    "\n",
    "\n",
    "# run example\n",
    "if __name__ == \"__main__\":\n",
    "    # rating matrix - User X Item : (7 X 5)\n",
    "    R = np.array([\n",
    "        [1, 0, 0, 1, 3],\n",
    "        [2, 0, 3, 1, 1],\n",
    "        [1, 2, 0, 5, 0],\n",
    "        [1, 0, 0, 4, 4],\n",
    "        [2, 1, 5, 4, 0],\n",
    "        [5, 1, 5, 4, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "    ])\n",
    "\n",
    "    # P, Q is (7 X k), (k X 5) matrix\n",
    "    factorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\n",
    "    factorizer.fit()\n",
    "    print(factorizer.get_complete_matrix())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
